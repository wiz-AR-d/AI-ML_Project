# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GSoDoXCx_tx-Ytn4ktPCYSH_3VObVR4S
"""

import json
import pandas as pd
import kagglehub

path = kagglehub.dataset_download("Cornell-University/arxiv")
file_path = path + "/arxiv-metadata-oai-snapshot.json"

data = []

with open(file_path, "r") as f:
    for i, line in enumerate(f):
        if i >= 20000:   # Load first 20k safely
            break
        data.append(json.loads(line))

df = pd.DataFrame(data)

df.head()

"""# Task
Preprocess the 'abstract' column of the `df` DataFrame by converting text to lowercase, removing punctuation, and then tokenizing the cleaned text into individual words. Display the original and tokenized 'abstract' columns for verification and discuss the preprocessing steps and any observations from the processed data.

## Clean Abstract Text

### Subtask:
Define a Python function to preprocess text by converting it to lowercase and removing punctuation. This function will then be applied to the 'abstract' column of the DataFrame to create a new cleaned column.

**Reasoning**:
First, I'll define a Python function to preprocess text by converting it to lowercase and removing punctuation. Then, I'll apply this function to the 'abstract' column of the DataFrame to create a new cleaned column, 'cleaned_abstract'.
"""

import string

def clean_text(text):
    if isinstance(text, str):
        text = text.lower()
        text = text.translate(str.maketrans('', '', string.punctuation))
    else:
        text = '' # Handle non-string values gracefully, e.g., None or NaN
    return text

df['cleaned_abstract'] = df['abstract'].apply(clean_text)

print(df[['abstract', 'cleaned_abstract']].head())

"""## Tokenize Abstract Text

### Subtask:
Apply the cleaning function to the 'abstract' column and then tokenize the cleaned text into individual words, storing the list of tokens in a new column.

**Reasoning**:
I will tokenize the 'cleaned_abstract' column by splitting the text into words based on spaces and store the result in a new column named 'tokenized_abstract'.
"""

df['tokenized_abstract'] = df['cleaned_abstract'].apply(lambda x: x.split() if isinstance(x, str) else [])

print(df[['abstract', 'cleaned_abstract', 'tokenized_abstract']].head())

"""## Final Task

### Subtask:
Summarize the preprocessing steps performed on the 'abstract' column, including lowercasing, punctuation removal, and tokenization, and discuss any immediate observations from the processed data.

## Summary:

### Q&A
The preprocessing steps performed on the 'abstract' column involved three main stages:
1.  **Lowercasing**: All text in the 'abstract' column was converted to lowercase to ensure consistency.
2.  **Punctuation Removal**: All punctuation marks were removed from the text to clean it further.
3.  **Tokenization**: The cleaned text was then split into individual words (tokens).

Immediate observations from the processed data include the successful transformation of original text like "A fully differential calculation..." to "a fully differential calculation..." after cleaning, and "$ (k,\\ell)$-" becoming "kell-", indicating effective removal of capitalization and punctuation. The final output also shows abstracts as lists of individual words, confirming successful tokenization.

### Data Analysis Key Findings
*   The 'abstract' column underwent a systematic preprocessing pipeline, including lowercasing, punctuation removal, and tokenization.
*   The `clean_text` function effectively converted all abstract text to lowercase and stripped out punctuation; for instance, the original phrase "A fully differential calculation..." became "a fully differential calculation...", and "$ (k,\\ell)$-" was transformed into "kell-".
*   A new column, `cleaned_abstract`, was created to store the text after lowercasing and punctuation removal.
*   The `cleaned_abstract` column was subsequently tokenized, resulting in a `tokenized_abstract` column where each abstract is represented as a list of individual words.

### Insights or Next Steps
*   The current preprocessing steps have successfully standardized the 'abstract' text, making it suitable for subsequent Natural Language Processing (NLP) tasks.
*   The tokenized abstract data is now in a format ready for further text analysis, such as stop word removal, stemming/lemmatization, or feature extraction for machine learning models.

# Task
Remove stop words from the `tokenized_abstract` column by first downloading NLTK stop words, then filtering out these stop words, and storing the result in a new column named `abstract_no_stopwords`.

## Remove Stop Words

### Subtask:
Download NLTK stop words if not already available, then remove common stop words from the `tokenized_abstract` column, storing the result in a new column, `abstract_no_stopwords`.

**Reasoning**:
I will import the `nltk` library, download the 'stopwords' corpus, load English stop words, define a function to remove stop words from a list of tokens, apply this function to the `tokenized_abstract` column, and then display the original, tokenized, and stop-word-removed abstracts.
"""

import nltk
nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
    if isinstance(tokens, list):
        return [word for word in tokens if word not in stop_words]
    return []

df['abstract_no_stopwords'] = df['tokenized_abstract'].apply(remove_stopwords)

print(df[['abstract', 'tokenized_abstract', 'abstract_no_stopwords']].head())

"""## Lemmatize Tokens

### Subtask:
Download NLTK wordnet if not already available, initialize a WordNetLemmatizer, and then apply lemmatization to the `abstract_no_stopwords` column, storing the result in a new column, `lemmatized_abstract`.

**Reasoning**:
I will import the `nltk` library, download the 'wordnet' corpus, initialize a `WordNetLemmatizer`, define a function to lemmatize a list of tokens, and then apply this function to the `abstract_no_stopwords` column to create a new `lemmatized_abstract` column. Finally, I will display the relevant columns to verify the lemmatization.
"""

import nltk
nltk.download('wordnet', quiet=True)
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
    if isinstance(tokens, list):
        return [lemmatizer.lemmatize(word) for word in tokens]
    return []

df['lemmatized_abstract'] = df['abstract_no_stopwords'].apply(lemmatize_tokens)

print(df[['abstract_no_stopwords', 'lemmatized_abstract']].head())

"""## Display All Processed Abstracts

### Subtask:
Display the original 'abstract', 'cleaned_abstract', 'tokenized_abstract', 'abstract_no_stopwords', and 'lemmatized_abstract' columns to show the step-by-step transformation.

**Reasoning**:
To display the step-by-step transformation of the 'abstract' column, I will print the first 5 rows of the DataFrame, selecting all the relevant columns including 'abstract', 'cleaned_abstract', 'tokenized_abstract', 'abstract_no_stopwords', and 'lemmatized_abstract'.
"""

print(df[['abstract', 'cleaned_abstract', 'tokenized_abstract', 'abstract_no_stopwords', 'lemmatized_abstract']].head())

"""## Final Task

### Subtask:
Summarize all preprocessing steps performed (lowercasing, punctuation removal, tokenization, stop-word removal, and lemmatization) and discuss observations from the fully processed data.

## Summary:

### Q&A
The following preprocessing steps were performed: lowercasing, punctuation removal, tokenization, stop-word removal, and lemmatization.

Observations from the fully processed data indicate a significant reduction in text size and complexity, making the data more suitable for subsequent natural language processing tasks. The process effectively stripped away grammatical noise (stop words) and morphological variations (lemmatization), resulting in a cleaner and more standardized representation of the abstract content.

### Data Analysis Key Findings
*   **Lowercasing and Punctuation Removal:** The `cleaned_abstract` column demonstrates the application of lowercasing and removal of special characters from the original `abstract` text.
*   **Tokenization:** The `tokenized_abstract` column successfully transformed the cleaned text into a list of individual words or tokens.
*   **Stop-word Removal:** Common English stop words such as 'a', 'in', 'we', 'the', 'of', and 'is' were effectively removed from the `tokenized_abstract` column, resulting in the `abstract_no_stopwords` column.
*   **Lemmatization:** Tokens in the `abstract_no_stopwords` column were reduced to their base or root form; for example, the word "numbers" was lemmatized to "number" in the `lemmatized_abstract` column.
*   The final display of all processed columns (`abstract`, `cleaned_abstract`, `tokenized_abstract`, `abstract_no_stopwords`, `lemmatized_abstract`) clearly illustrates the step-by-step transformation and cleaning of the text data.

### Insights or Next Steps
*   The fully processed data, now free from common grammatical words and morphological variations, is significantly cleaner and more focused, making it ideal for quantitative analysis such as word frequency counts, topic modeling, or sentiment analysis.
*   Further analysis could involve generating word clouds from the `lemmatized_abstract` column to quickly visualize the most prominent terms, or calculating term frequency-inverse document frequency (TF-IDF) scores for feature engineering in machine learning models.

# Task
Join the lemmatized tokens in the `lemmatized_abstract` column into a single string for each abstract, creating a new column named `abstract_for_tfidf`. Display the first few entries of this new column alongside the `lemmatized_abstract` column to verify the transformation.

## Prepare Text for TF-IDF

### Subtask:
Join the lemmatized tokens in the `lemmatized_abstract` column into a single string for each abstract. This formatted text will be used for TF-IDF feature extraction.

**Reasoning**:
First, I'll define a function to join a list of tokens into a single string, handling non-list inputs gracefully. Then, I'll apply this function to the 'lemmatized_abstract' column to create the new 'abstract_for_tfidf' column, which will be suitable for TF-IDF feature extraction. Finally, I will display the first 5 rows of the DataFrame, showing the 'lemmatized_abstract' and 'abstract_for_tfidf' columns for verification.
"""

def join_tokens(tokens):
    if isinstance(tokens, list):
        return ' '.join(tokens)
    return ''

df['abstract_for_tfidf'] = df['lemmatized_abstract'].apply(join_tokens)

print(df[['lemmatized_abstract', 'abstract_for_tfidf']].head())

"""## TF-IDF Feature Extraction

### Subtask:
Apply TF-IDF (Term Frequency-Inverse Document Frequency) vectorization to the prepared text to transform the abstracts into a numerical feature matrix. This will allow for quantitative analysis of word importance.

**Reasoning**:
I will import TfidfVectorizer from sklearn, instantiate it, and then apply fit_transform to the 'abstract_for_tfidf' column to create the TF-IDF matrix.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(df['abstract_for_tfidf'])

print("TF-IDF matrix shape:", tfidf_matrix.shape)
print("First 5 TF-IDF features for the first abstract:\n", tfidf_matrix[0, :5].toarray())
print("Feature names (first 10):", tfidf_vectorizer.get_feature_names_out()[:10])

"""## Extractive Summarization

### Subtask:
Implement a basic extractive summarization technique. This will involve identifying and selecting key sentences from the original abstracts based on the TF-IDF scores of their constituent words to form a summary.

**Reasoning**:
First, I'll import necessary NLTK modules and download the 'punkt' tokenizer. Then, I'll define the `extractive_summarize` function that processes each abstract, tokenizes its sentences, cleans, removes stop words, lemmatizes, and scores them based on TF-IDF values before selecting the top sentences. Finally, I will apply this function to the DataFrame to create the new `extractive_summary` column and display the results.
"""

import nltk
nltk.download('punkt', quiet=True)
from nltk.tokenize import sent_tokenize

def extractive_summarize(abstract_text, abstract_index, num_sentences=3):
    if not isinstance(abstract_text, str) or pd.isna(abstract_text):
        return ""

    sentences = sent_tokenize(abstract_text)
    sentence_scores = []

    for sentence in sentences:
        cleaned_sentence = clean_text(sentence)
        tokens = cleaned_sentence.split()
        no_stopwords_tokens = remove_stopwords(tokens)
        lemmatized_tokens = lemmatize_tokens(no_stopwords_tokens)

        sentence_score = 0
        for word in lemmatized_tokens:
            if word in tfidf_vectorizer.vocabulary_:
                # Get the index of the word in the TF-IDF vocabulary
                word_idx = tfidf_vectorizer.vocabulary_[word]
                # Add the TF-IDF score for this word in this specific document
                sentence_score += tfidf_matrix[abstract_index, word_idx]

        sentence_scores.append((sentence_score, sentence))

    # Sort sentences by score in descending order
    sentence_scores.sort(key=lambda x: x[0], reverse=True)

    # Select the top N sentences for the summary
    summary_sentences = [s for score, s in sentence_scores[:num_sentences]]

    return ' '.join(summary_sentences)

# Apply the summarization function to each row
df['extractive_summary'] = df.apply(lambda row: extractive_summarize(row['abstract'], row.name, 3), axis=1)

# Display the original abstract and the generated summary
print(df[['abstract', 'extractive_summary']].head())

"""**Reasoning**:
The previous code failed because the NLTK `punkt_tab` resource was not found, as indicated by the `LookupError`. The error message explicitly suggested running `nltk.download('punkt_tab')`. I will add this download alongside the `punkt` download to resolve the issue, and also ensure `pandas` is imported for the `pd.isna` check.


"""

import nltk
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab', quiet=True) # Download 'punkt_tab' as suggested by the error message
from nltk.tokenize import sent_tokenize
import pandas as pd # Import pandas for pd.isna

def extractive_summarize(abstract_text, abstract_index, num_sentences=3):
    if not isinstance(abstract_text, str) or pd.isna(abstract_text):
        return ""

    sentences = sent_tokenize(abstract_text)
    sentence_scores = []

    for sentence in sentences:
        cleaned_sentence = clean_text(sentence)
        tokens = cleaned_sentence.split()
        no_stopwords_tokens = remove_stopwords(tokens)
        lemmatized_tokens = lemmatize_tokens(no_stopwords_tokens)

        sentence_score = 0
        for word in lemmatized_tokens:
            if word in tfidf_vectorizer.vocabulary_:
                # Get the index of the word in the TF-IDF vocabulary
                word_idx = tfidf_vectorizer.vocabulary_[word]
                # Add the TF-IDF score for this word in this specific document
                sentence_score += tfidf_matrix[abstract_index, word_idx]

        sentence_scores.append((sentence_score, sentence))

    # Sort sentences by score in descending order
    sentence_scores.sort(key=lambda x: x[0], reverse=True)

    # Select the top N sentences for the summary
    summary_sentences = [s for score, s in sentence_scores[:num_sentences]]

    return ' '.join(summary_sentences)

# Apply the summarization function to each row
df['extractive_summary'] = df.apply(lambda row: extractive_summarize(row['abstract'], row.name, 3), axis=1)

# Display the original abstract and the generated summary
print(df[['abstract', 'extractive_summary']].head())

"""## Display Summaries

### Subtask:
Display a few original abstracts alongside their generated extractive summaries to visually inspect the results of the summarization.

**Reasoning**:
To visually inspect the results, I will display the first few entries of the original 'abstract' column alongside the newly generated 'extractive_summary' column, as per the instructions.
"""

print(df[['abstract', 'extractive_summary']].head())

"""## Discuss Evaluation Measures

### Subtask:
Provide a detailed discussion on how coherence and interpretability of the generated summaries can be evaluated, considering both automated metrics and human assessment.

## Discuss Evaluation Measures

### Subtask:
Provide a detailed discussion on how coherence and interpretability of the generated summaries can be evaluated, considering both automated metrics and human assessment.

#### 1. Automated Metrics
Automated metrics compare a generated summary against one or more human-written reference summaries. While they offer speed and consistency, they often struggle to capture semantic meaning and overall summary quality.

*   **ROUGE (Recall-Oriented Understudy for Gisting Evaluation):** ROUGE is a set of metrics that measure the overlap of n-grams (sequences of words) between the generated summary and the reference summary. It primarily assesses **recall** (how much of the reference summary is captured by the generated summary). Different variants exist:
    *   **ROUGE-N:** Measures the overlap of N-gram units (e.g., ROUGE-1 for unigrams, ROUGE-2 for bigrams). Higher scores indicate more shared words or phrases.
    *   **ROUGE-L:** Measures the longest common subsequence (LCS) between the generated and reference summaries. This metric doesn't require consecutive matches, allowing for flexibility in word order and better reflecting sentence-level similarity.
    *   **Relevance to Extractive Summaries:** ROUGE is highly relevant for extractive summaries as it evaluates how well the extracted sentences match the key information present in reference summaries. High ROUGE scores suggest that the summary has extracted important content.

*   **METEOR (Metric for Evaluation of Translation with Explicit Ordering):** METEOR goes beyond n-gram overlap by considering synonyms and stemming through WordNet, as well as an explicit word alignment between the summary and the reference. It also includes a penalty for incorrect word order. This makes METEOR a more robust metric for evaluating **precision, recall, and a form of fluency/alignment**.
    *   **Relevance to Extractive Summaries:** METEOR can be useful for extractive summaries by identifying if the chosen sentences not only match content but also if semantically similar words are used, which could occur if the extraction process implicitly favors certain terms over others that are contextually important.

*   **BLEU (Bilingual Evaluation Understudy):** Originally developed for machine translation, BLEU measures the **precision** of n-grams in the generated text compared to reference texts. It looks at how many n-grams in the candidate summary appear in the reference summary, with a brevity penalty for short summaries. While less directly applicable to summarization than ROUGE, it can give an indication of how precisely the generated summary matches phrases in the reference.
    *   **Relevance to Extractive Summaries:** BLEU can provide insights into the exactness of extracted phrases. A higher BLEU score might indicate that the extractive summary has precisely chosen sentences or phrases that are also present in a human-created summary.

#### 2. Human Assessment - Coherence
Coherence refers to how well the sentences in a summary flow together and form a logically consistent text. It addresses whether the summary makes sense as a standalone piece of writing.

*   **Evaluation Method:** Human judges read the generated summaries and rate them based on how well the sentences connect logically and semantically.
*   **Rubric/Scale (e.g., 1-5):**
    *   **1 (Very Incoherent):** Sentences are disjointed, abrupt topic shifts, very difficult to understand the overall message.
    *   **2 (Somewhat Incoherent):** Some logical breaks, occasional abrupt transitions, requires effort to follow the main points.
    *   **3 (Acceptable Coherence):** Generally flows well, minor breaks in logic, easy to understand the main points.
    *   **4 (Good Coherence):** Sentences flow smoothly, logical progression of ideas, easy to read and understand.
    *   **5 (Excellent Coherence):** Flawless sentence transitions, strong logical structure, reads like a perfectly written summary.

#### 3. Human Assessment - Interpretability
Interpretability assesses whether the summary accurately reflects the main points of the original abstract, is easy to understand, and is free from ambiguity or factual errors.

*   **Evaluation Method:** Human judges compare the generated summary against the original abstract to determine if it accurately conveys the core message, is understandable, and free from misinterpretations or factual inaccuracies introduced during summarization.
*   **Rubric/Scale (e.g., 1-5):**
    *   **1 (Very Poor Interpretability):** Fails to capture main points, misleading, full of factual errors or ambiguities, difficult to understand.
    *   **2 (Poor Interpretability):** Misses key information, contains some errors or ambiguities, hard to grasp the essence of the original.
    *   **3 (Acceptable Interpretability):** Captures most main points, generally clear, minor ambiguities or omissions.
    *   **4 (Good Interpretability):** Accurately reflects main points, clear and concise, easy to understand with no significant errors.
    *   **5 (Excellent Interpretability):** Perfectly captures all main points, highly lucid, completely free from ambiguity or factual errors, enhances understanding of the original.

#### 4. Challenges and Considerations
Evaluating summarization is inherently complex due to several factors:

*   **Subjectivity:** What constitutes a 'good' summary is often subjective. Different human annotators might emphasize different aspects of an abstract, leading to variations in reference summaries and human judgments.
*   **Difficulty in Obtaining Reliable Human Annotations:** Human annotation is time-consuming and expensive. Achieving high inter-annotator agreement for metrics like coherence and interpretability can be challenging, as human perceptions vary.
*   **Limitations of Automated Metrics:** While useful, automated metrics have significant limitations:
    *   **Lack of Semantic Understanding:** They primarily rely on lexical overlap and cannot truly understand the meaning or intent behind the words. A summary might have high ROUGE scores but still be incoherent or factually incorrect.
    *   **Insensitivity to Fluency and Coherence:** Metrics like ROUGE do not directly evaluate how smoothly sentences flow together or the overall logical structure of the summary.
    *   **Dependency on Reference Summaries:** Automated metrics require high-quality reference summaries, which are often scarce and expensive to create. An extractive summary might be good but perform poorly if it uses different phrasing or extracts different (but equally valid) sentences than the reference.
    *   **Difficulty with Novelty:** If a summary uses paraphrasing or synthesizes information in a novel way (more common in abstractive summarization but can still affect how extractive summaries are evaluated against strict references), automated metrics might penalize it.

## Final Task

### Subtask:
Summarize the entire process, including feature extraction, summarization, and potential evaluation methods, and discuss insights gained from these steps.

## Summary:

### Data Analysis Key Findings

*   **Text Preparation for TF-IDF**: Lemmatized tokens from the `lemmatized_abstract` column were successfully joined into single strings, creating a new `abstract_for_tfidf` column, which is suitable for TF-IDF processing.
*   **TF-IDF Feature Extraction**: TF-IDF vectorization was applied, transforming 20,000 abstract documents into a numerical feature matrix (`tfidf_matrix`) with 73,221 unique terms.
*   **Extractive Summarization Implementation**: A basic extractive summarization technique was successfully implemented. This involved:
    *   Tokenizing abstracts into sentences.
    *   Cleaning, stop-word removal, and lemmatization of words within each sentence.
    *   Scoring sentences based on the TF-IDF scores of their constituent words.
    *   Selecting the top 3 scoring sentences to form the summary, which were then stored in a new `extractive_summary` column.
    *   NLTK's `punkt` and `punkt_tab` resources were utilized for sentence tokenization.
*   **Visual Verification of Summaries**: The `extractive_summary` column was displayed alongside the original `abstract` column, allowing for visual inspection and confirmation of the generated summaries.
*   **Comprehensive Evaluation Discussion**: A detailed discussion on evaluating summary quality was provided, covering:
    *   **Automated Metrics**: ROUGE (Recall-Oriented Understudy for Gisting Evaluation), METEOR (Metric for Evaluation of Translation with Explicit Ordering), and BLEU (Bilingual Evaluation Understudy), along with their relevance to extractive summarization.
    *   **Human Assessment for Coherence**: A 1-5 rubric was proposed to evaluate the logical flow and topic transitions within a summary.
    *   **Human Assessment for Interpretability**: A 1-5 rubric was suggested to assess how accurately, clearly, and unambiguously the summary reflects the original content.
    *   **Challenges and Considerations**: The discussion highlighted the subjectivity of summarization, the difficulty in obtaining reliable human annotations, and the inherent limitations of automated metrics (e.g., lack of semantic understanding, insensitivity to fluency, dependency on reference summaries).

### Insights or Next Steps

*   The implemented extractive summarization method, while basic, provides a foundational approach that can be enhanced by integrating more sophisticated text processing techniques or incorporating semantic similarity measures beyond simple TF-IDF scores for sentence scoring.
*   To thoroughly evaluate the effectiveness of the generated summaries, it is crucial to apply the discussed human assessment rubrics for coherence and interpretability, potentially complemented by automated metrics (like ROUGE) against human-written reference summaries if available.
"""